services:
  spark-master:
    image: docker.io/bitnami/spark:3.5.3
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - type: bind
        source: ./conf/log4j.properties
        target: /opt/bitnami/spark/conf/log4j.properties
    ports:
      - '8080:8080'
      - '7077:7077'
    networks:
      - spark
    container_name: spark

  spark-worker-1:
    image: docker.io/bitnami/spark:3.5.3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=8G
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - type: bind
        source: ./conf/log4j.properties
        target: /opt/bitnami/spark/conf/log4j.properties
    ports:
      - '8081:8081'
    container_name: spark-worker
    networks:
      - spark
    depends_on:
      - spark-master

  namenode:
    image: apache/hadoop:3.3.5
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./volumes/hadoop_namenode:/opt/hadoop/data/nameNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/start-hdfs.sh:/start-hdfs.sh
      - ./data/:/tmp/data/
    ports:
      - "9870:9870"
    command: [ "/bin/bash", "/start-hdfs.sh" ]
    networks:
      - spark
    healthcheck:
      test: [ "CMD", "/opt/hadoop/bin/hdfs", "dfs", "-ls", "/input" ]
      interval: 10s
      timeout: 5s
      retries: 5

  datanode:
    image: apache/hadoop:3.3.5
    container_name: datanode
    hostname: datanode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./volumes/hadoop_datanode:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/init-datanode.sh:/init-datanode.sh
    depends_on:
      - namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    networks:
      - spark

  app:
    build:
      context: app
    ports:
      - '5000:5000'
    container_name: app
    volumes:
      - type: bind
        source: ./analysis
        target: /usr/src/analysis
    networks:
      - spark
    depends_on:
      namenode:
          condition: service_healthy
      spark-master:
          condition: service_started



networks:
  spark:
    driver: bridge

volumes:
  hdfs-data: